{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fr1ipBUMKGVf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AlbertTokenizer\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import networkx as nx\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# Function to create an edge index from text data\n",
        "def create_edge_index_from_text(df):\n",
        "    G = nx.Graph()\n",
        "    for text in df['text']:\n",
        "        words = text.split()\n",
        "        for i in range(len(words)):\n",
        "            for j in range(i + 1, len(words)):\n",
        "                if G.has_edge(words[i], words[j]):\n",
        "                    G[words[i]][words[j]]['weight'] += 1\n",
        "                else:\n",
        "                    G.add_edge(words[i], words[j], weight=1)\n",
        "    edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().contiguous()\n",
        "    return edge_index\n",
        "\n",
        "# Load datasets\n",
        "data1 = {\n",
        "    'text': [\n",
        "        \"پاکستان میں لاک ڈاؤن عمران کی حکومت نے لگایا اور 2 کروڑ لوگوں کو بے روزگار کیا۔\",\n",
        "        \"چین نے محفوظ فضائی سفرکے لیے کوویڈ-19سے بچاو کے اقدامات سخت کردئے\"\n",
        "    ],\n",
        "    'label': [1, 0]\n",
        "}\n",
        "\n",
        "data2 = {\n",
        "    'text': [\n",
        "        \"چین کی جانب سے پاکستان کی مدد کرنے سے انکار کا اعلان۔\",\n",
        "        \"1 لاکھ 29 لوگوں نے روبوٹس کی جگہ لے لی کرونا وائرس کی وجہ سے۔\"\n",
        "    ],\n",
        "    'label': [1, 0]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df1 = pd.DataFrame(data1)\n",
        "df2 = pd.DataFrame(data2)\n",
        "\n",
        "# Combine datasets\n",
        "datasets = pd.concat([df1, df2])\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "tokenized_data = tokenizer(datasets['text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Create graph data\n",
        "edge_index = create_edge_index_from_text(datasets)\n",
        "x = torch.tensor(tokenized_data['input_ids'], dtype=torch.float)  # Node features\n",
        "y = torch.tensor(datasets['label'].tolist(), dtype=torch.long)  # Labels\n",
        "\n",
        "graph_data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "# Create train and test split\n",
        "train_data, test_data = train_test_split([graph_data], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
        "Model Implementation with Multi-Head Attention and Prompt Learning\n",
        "python\n",
        "Copy code\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv\n",
        "from transformers import AlbertModel\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.multihead_attn(x, x, x)\n",
        "        return attn_output\n",
        "\n",
        "class GCAWithPromptLearning(nn.Module):\n",
        "    def __init__(self, num_classes, gcn_input_dim, gcn_hidden_dim, albert_model_name, num_heads, attention_embed_dim, prompt_length):\n",
        "        super(GCAWithPromptLearning, self).__init__()\n",
        "        self.gcn1 = GCNConv(gcn_input_dim, gcn_hidden_dim)\n",
        "        self.gcn2 = GCNConv(gcn_hidden_dim, gcn_hidden_dim)\n",
        "        self.albert = AlbertModel.from_pretrained(albert_model_name)\n",
        "        self.multihead_attn = MultiHeadAttention(attention_embed_dim, num_heads)\n",
        "        self.prompt_length = prompt_length\n",
        "        self.fc1 = nn.Linear(self.albert.config.hidden_size + gcn_hidden_dim + prompt_length, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, x, edge_index, prompt):\n",
        "        # GCN forward\n",
        "        x = self.gcn1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.gcn2(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        # ALBERT forward\n",
        "        albert_outputs = self.albert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        albert_cls_output = albert_outputs[1]  # Get the [CLS] token representation\n",
        "\n",
        "        # Multi-head attention\n",
        "        attn_output = self.multihead_attn(albert_cls_output.unsqueeze(0))\n",
        "        attn_output = attn_output.squeeze(0)\n",
        "\n",
        "        # Concatenate ALBERT output with GCN output and prompt\n",
        "        combined = torch.cat((attn_output, x, prompt), dim=1)\n",
        "        combined = torch.relu(self.fc1(combined))\n",
        "        logits = self.fc2(combined)\n",
        "\n",
        "        return logits\n",
        "Training and Evaluation Code\n",
        "python\n",
        "Copy code\n",
        "import torch.optim as optim\n",
        "\n",
        "# Create prompt tokens\n",
        "def create_prompt_tokens(prompt_length, tokenizer):\n",
        "    prompt_text = \" \".join([\"[MASK]\"] * prompt_length)\n",
        "    prompt_tokens = tokenizer(prompt_text, return_tensors='pt')['input_ids']\n",
        "    return prompt_tokens\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "prompt_length = 5  # Example prompt length\n",
        "prompt_tokens = create_prompt_tokens(prompt_length, tokenizer)\n",
        "model = GCAWithPromptLearning(num_classes=2, gcn_input_dim=768, gcn_hidden_dim=128, albert_model_name='albert-base-v2', num_heads=8, attention_embed_dim=768, prompt_length=prompt_length)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(10):  # Number of epochs\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch.x  # Tokenized input_ids\n",
        "        attention_mask = (input_ids != tokenizer.pad_token_id).float()  # Generate attention mask\n",
        "        x = batch.x\n",
        "        edge_index = batch.edge_index\n",
        "        labels = batch.y\n",
        "        prompt = prompt_tokens.repeat(input_ids.size(0), 1)  # Repeat prompt for batch size\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask, x, edge_index, prompt)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch.x\n",
        "        attention_mask = (input_ids != tokenizer.pad_token_id).float()\n",
        "        x = batch.x\n",
        "        edge_index = batch.edge_index\n",
        "        labels = batch.y\n",
        "        prompt = prompt_tokens.repeat(input_ids.size(0), 1)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask, x, edge_index, prompt)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {94 * correct / total}%')"
      ]
    }
  ]
}